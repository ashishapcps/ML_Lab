{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2QyEAerMmsg",
        "colab_type": "text"
      },
      "source": [
        "# **MLDM LAB 9**\n",
        "For this coursework you are required to write your own implementation of the backpropagation\n",
        "algorithm for training a neural network.\n",
        "You are required to do this assignment in the Python (Python version 3) programming language. Do\n",
        "not use any audodiff toolboxes (TensorFlow, Keras, PyTorch, etc) - you are allowed to use only\n",
        "numpy like libraries (numpy, pandas, etc).\n",
        "The goal of this assignment is to label images of 10 handwritten digits of ‚Äúzero‚Äù, ‚Äúone‚Äù,...,‚Äúnine‚Äù. The\n",
        "images are 28 by 28 in size (MNIST dataset), which we will be represented as a vector x of\n",
        "dimension 784 by listing all the pixel values in raster scan order. The labels are 0,1,2,...,9\n",
        "corresponding to 10 classes as written in the image. There are 3000 training cases, containing 300\n",
        "examples of each of 10 classes.\n",
        "The way you choose to design your code for this homework will affect how much time you\n",
        "spend coding. We recommend that you look through all of the problems before attempting the first\n",
        "problem. A good foundation will make the rest of these problems easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyOfL9FKDoG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlcq1HAs7meI",
        "colab_type": "text"
      },
      "source": [
        "##Problem 1\n",
        "Here you must read an input file. Each line contains 785 numbers (comma delimited): the first\n",
        "values are between 0.0 and 1.0 correspond to the 784 pixel values (black and white images), and the\n",
        "last number denotes the class label: 0 corresponds to digit 0, 1 corresponds to digit 1, etc.\n",
        "As a warm up question, load the data. For this problem you must write a function that takes a file\n",
        "path as an argument which contains this data. Your function must return two values (X and Y) that\n",
        "contains the data from the file as described. Specifically, the first return value (X) must be a matrix\n",
        "where the rows are individual examples of images, and the columns are individual pixels (N x 784\n",
        "matrix). The second return value must be a list/array of real numbers representing the labels of the\n",
        "examples (rows) in X.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USiZZJaUFQcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  df.head()\n",
        "  Y = df['5'].to_numpy()\n",
        "  del df['5']\n",
        "  X=df.to_numpy()\n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXIEJCvD0dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_data(\"mnist_train.csv\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "#One hot encoding of training labels \n",
        "Labels=pd.get_dummies(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zyoHKGD7tvJ",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2\n",
        "Implement the backpropagation algorithm in a zero hidden layer neural network (weights\n",
        "between input and output nodes). The output layer should be a softmax output over 10 classes\n",
        "corresponding to 10 classes of handwritten digits (e.g. an architecture: 784 > 10). Your backprop\n",
        "code\n",
        "should minimize the cross-entropy entropy function for multi-class classification problem\n",
        "(categorical cross entropy).\n",
        "ùêøùëúùë†ùë† = ‚àí Œ£ùë°ùëéùëüùëîùëíùë°ùëó ‚àó log (ùëùùëüùëíùëëùëñùëêùë°ùëñùëúùëõùëó)\n",
        "ùëó\n",
        "where j is the class label\n",
        "This step should be done with a full step of gradient descent.\n",
        "For this problem you must write a function that takes as an input a matrix of X values, a list of Y\n",
        "values (as returned from problem 1), a weight matrix, and a learning rate and performs a single step\n",
        "of backpropagation. You will need to do both a forward step with the inputs, and then a backward\n",
        "prop to get the gradients. Return the updated weight matrix and bias in the same format as it was\n",
        "passed.\n",
        "The list of weight matrices will be a list with 1 entry where the only entry is a matrix in the format\n",
        "where the rows represent all of the outgoing weights for a neuron in the input layer and the\n",
        "columns represent the weights for the incoming neurons. A specific row column index will give you\n",
        "the weight for a neuron to neuron connection.\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same\n",
        "length as the first set of weights. (e.g. for an architecture of 784 > 10, there will be a single element\n",
        "list with a vector of size 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aYMsXGyDwqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            x=self.softmax(z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            prev_term=self.delta_mll(y,self.y_pred)  \n",
        "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            x=self.softmax(z) \n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjgzI7Ss7dbD",
        "colab_type": "code",
        "outputId": "977bf29a-a503-4cb5-c8ea-0bf5c7b3e357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "n=Perceptron(X_train,Labels)\n",
        "n.connect(X_train,Labels)\n",
        "n.train(batches=1000,lr=0.2,epoch=30)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.00010348993840767894\n",
            "1 0.0010112675672053182\n",
            "2 0.0010907000862511091\n",
            "3 0.0010131414411530507\n",
            "4 0.0011390485726355108\n",
            "5 0.001006482643722832\n",
            "6 0.0009895918755949522\n",
            "7 0.0009185293919431851\n",
            "8 0.0007632285245876506\n",
            "9 0.0006724267834421\n",
            "10 0.0005054539188598704\n",
            "11 0.0003833546798350255\n",
            "12 0.00030685219773915674\n",
            "13 0.00024515665123263814\n",
            "14 0.0002267697213364515\n",
            "15 0.00020035533518292519\n",
            "16 0.0001778636991965888\n",
            "17 0.00014865098815978125\n",
            "18 0.00011600270833004023\n",
            "19 0.0001045353595027113\n",
            "20 9.572970240515608e-05\n",
            "21 9.241425364495557e-05\n",
            "22 9.378329883088633e-05\n",
            "23 9.030948569225381e-05\n",
            "24 8.655592735312674e-05\n",
            "25 8.620202894308977e-05\n",
            "26 8.782555910225113e-05\n",
            "27 8.908721535913005e-05\n",
            "28 8.920905018497918e-05\n",
            "29 8.856503065285183e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhDzZkLr-Bg0",
        "colab_type": "code",
        "outputId": "bad4fac2-c3cc-4633-b2f1-3f8a2e3f4345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([313, 332, 292, 343, 326, 267, 296, 317, 224, 290]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2jK6jKC-Il3",
        "colab_type": "code",
        "outputId": "681d359f-c661-459a-a76f-91652f9bbddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 88.23333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klZhm_WFDeV-",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3\n",
        "Extend your code from problem 2 to support a single layer neural network with N hidden units (e.g.\n",
        "an architecture: 784 > 10 > 10). These hidden units should be using sigmoid activations.\n",
        "For this problem you must write a function that takes as an input a matrix of X values, a list of Y\n",
        "values (as returned from problem 1), list of weight matrices, a list of bias vectors, a list of bias\n",
        "vectors, and a learning rate and performs a single step of backpropagation. You will need to do both\n",
        "a forward step with the inputs to get the outputs, and then a backward prop to get the gradients.\n",
        "Return the updated weight matrix and bias in the same format as it was passed.\n",
        "The list of weight matrices is a list with 2 entries where each entry in the list contains a single\n",
        "weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 the passed\n",
        "list of weight matrices would look like this: [Matrix with shape 784x10, Matrix with shape 10x10].\n",
        "Note: Though a hidden layer of size 10 is used as an example here, your code must be able to\n",
        "support a hidden layer of dimension N.\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same\n",
        "length as the first set of weights. (e.g. for an architecture of 784 > 10 > 10, there will be a two\n",
        "element list with an vector of size 10 and a vector of size 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYE-TondDhAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class SingleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "\n",
        "        return np.argmax(x,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wffR59W-D9zR",
        "colab_type": "code",
        "outputId": "1d885fe9-184c-462f-86ee-91f98b6f457d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=50)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0009388891457769082\n",
            "1 0.0005616333662782794\n",
            "2 0.00036126080374179113\n",
            "3 0.0002666369858463061\n",
            "4 0.00021839182229056034\n",
            "5 0.00019141150774094632\n",
            "6 0.00017059561660972414\n",
            "7 0.00014856444040118146\n",
            "8 0.00013141555747670736\n",
            "9 0.0001185449816296973\n",
            "10 0.00010894019944070337\n",
            "11 0.00010277150756337235\n",
            "12 9.844495861840572e-05\n",
            "13 9.354585788737808e-05\n",
            "14 8.673308112415881e-05\n",
            "15 7.910135307494447e-05\n",
            "16 7.167385940509534e-05\n",
            "17 6.477727686719125e-05\n",
            "18 5.853513856475222e-05\n",
            "19 5.295472453578525e-05\n",
            "20 4.7974568021909414e-05\n",
            "21 4.356403127425275e-05\n",
            "22 3.9708677257776505e-05\n",
            "23 3.636351797876325e-05\n",
            "24 3.34632770179213e-05\n",
            "25 3.094003705591005e-05\n",
            "26 2.8731782876705134e-05\n",
            "27 2.6785472743699996e-05\n",
            "28 2.50573354706747e-05\n",
            "29 2.3511841289595557e-05\n",
            "30 2.2120290624184334e-05\n",
            "31 2.0859785723773708e-05\n",
            "32 1.9713361932184668e-05\n",
            "33 1.8671389768535184e-05\n",
            "34 1.7730166445024208e-05\n",
            "35 1.6881677274711398e-05\n",
            "36 1.610859644694923e-05\n",
            "37 1.5395713570838246e-05\n",
            "38 1.473494832091023e-05\n",
            "39 1.4121706991852346e-05\n",
            "40 1.3552305089008358e-05\n",
            "41 1.302327822669424e-05\n",
            "42 1.253132071563689e-05\n",
            "43 1.2073328832166687e-05\n",
            "44 1.1646430960262388e-05\n",
            "45 1.1247994812384402e-05\n",
            "46 1.0875620048301638e-05\n",
            "47 1.0527123691302514e-05\n",
            "48 1.0200522775773341e-05\n",
            "49 9.894016567745884e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp0shsvSEWvN",
        "colab_type": "code",
        "outputId": "ae928641-7d18-42ab-8179-b6ae48d04c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([300, 327, 298, 331, 303, 279, 299, 335, 248, 280]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knkPcfW8EYx5",
        "colab_type": "code",
        "outputId": "26f1d425-aad7-4c03-b110-07e6d6d9cb41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.26666666666667 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcxS5MRXF1km",
        "colab_type": "text"
      },
      "source": [
        "## Problem 4\n",
        "Extend your code from problem 3 (use cross entropy error) and implement a 2-layer neural\n",
        "network, starting with a simple architecture containing N hidden units in each layer (e.g. with\n",
        "architecture: 784 > 10 > 10 > 10). These hidden units should be using sigmoid activations.\n",
        "For this problem you must write a function that takes as an input a matrix of X values, a list of Y\n",
        "values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate\n",
        "and performs a single step of backpropagation. You will need to do both a forward step with the\n",
        "inputs to get the outputs, and then a backward prop to get the gradients. Return the updated weight\n",
        "matrix and bias in the same format as it was passed.\n",
        "The list of weight matrices is a list with 3 entries where each entry in the list contains a single\n",
        "weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 > 10 the\n",
        "passed list of weight matrices would look like this: [Matrix with shape 784x10, Matrix with shape\n",
        "10x10,Matrix with shape 10x10].\n",
        "Note: Though a hidden layer of size 10 is used as an example here, your code must be able to\n",
        "support a hidden layer of dimension N.\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same\n",
        "length as the first set of weights. (e.g. for an architecture of 784 > 10 > 10, there will be a two\n",
        "element list with an vector of size 10 and a vector of size 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V_nmBd8mF4d5",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class DoubleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fa68ab9c-5e95-43da-b847-bf0b0a5fbae8",
        "id": "xJY6j6AiGCI3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "l2=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0010755882158452249\n",
            "1 0.000661553221538444\n",
            "2 0.0005848410589323713\n",
            "3 0.0005674125791942182\n",
            "4 0.0005487562226210539\n",
            "5 0.0004958409161343915\n",
            "6 0.0004535993093748583\n",
            "7 0.0004338007665462461\n",
            "8 0.00041892821635395786\n",
            "9 0.0003815193505393336\n",
            "10 0.0003276752789126197\n",
            "11 0.00026901378566166075\n",
            "12 0.0002148455496090269\n",
            "13 0.00016930188955539043\n",
            "14 0.0001328669127938403\n",
            "15 0.00010682787034030628\n",
            "16 9.000052177325366e-05\n",
            "17 7.829868462056554e-05\n",
            "18 6.955884714347984e-05\n",
            "19 6.281699067301146e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRAWf6C5GJgN",
        "colab_type": "code",
        "outputId": "be4e9428-c7c4-43e7-c6e4-d7919812d0a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([308, 331, 316, 331, 306, 257, 289, 332, 255, 275]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpGuKUbGMJ1",
        "colab_type": "code",
        "outputId": "4b549365-be99-43dd-d585-b844434586d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 90.56666666666666 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCuNTnWEGOub",
        "colab_type": "text"
      },
      "source": [
        "## Problem 5\n",
        "Extend your code from problem 4 to implement different activations functions which will be\n",
        "passed as a parameter. In this problem all activations (except the final layer which should remain a\n",
        "softmax) must be changed to the passed activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I-KrbHw9GlsV",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkActivations():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "39ed7ba2-421c-4f43-e413-22e11c7822d4",
        "id": "JjlQBHZCHFWA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "n=NeuralNetworkActivations(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0008268394385604774\n",
            "1 0.0008374240033978214\n",
            "2 0.0011971206007448732\n",
            "3 0.00078511430221587\n",
            "4 0.0005246913151490196\n",
            "5 0.0003400391621533457\n",
            "6 0.0003206289622290754\n",
            "7 7.481283102607701e-05\n",
            "8 0.00015948195221391262\n",
            "9 0.0003719206494975074\n",
            "10 0.0001335959107262124\n",
            "11 0.0001539683466047304\n",
            "12 6.707591970206853e-05\n",
            "13 1.4997156867745919e-05\n",
            "14 5.1750913604735366e-05\n",
            "15 0.0009712169790221077\n",
            "16 0.0003198721253740984\n",
            "17 0.0006797670783904125\n",
            "18 0.0007174575181597092\n",
            "19 2.779925513372009e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZfek1XaL2PR",
        "colab_type": "code",
        "outputId": "1073d5e1-2eb0-4166-8d72-6013c2e5ba71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([306, 326, 305, 337, 318, 247, 284, 334, 259, 284]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2UmE1jNEUV",
        "colab_type": "code",
        "outputId": "a049a8a2-64b1-4698-cc80-860c75ff04ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.26666666666667 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFVOwq1RQLVo",
        "colab_type": "text"
      },
      "source": [
        "## Problem 6\n",
        "Extend your code from problem 5 to implement momentum with your gradient descent. The\n",
        "momentum value will be passed as a parameter. Your function should perform ‚Äúepoch‚Äù number of\n",
        "epochs and return the resulting weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IyyRnpPaQRSS",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkMomentum():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        self.delta_weights=[]\n",
        "        self.delta_bias=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "            \n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr,beta=0.9,momentum=False):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            if momentum:\n",
        "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
        "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
        "            else:\n",
        "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,beta,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr,beta,momentum=True)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9982e11e-cc7a-479e-ba8c-b8e9cd2a4210",
        "id": "5KieS6HyQuwx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "n=NeuralNetworkMomentum(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=500,lr=0.1,epoch=20,beta=0.5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0005570469709048465\n",
            "1 0.0009574010825463601\n",
            "2 0.0009300276907506841\n",
            "3 0.0005264203377113282\n",
            "4 0.000681485513329798\n",
            "5 0.00013805642631930606\n",
            "6 0.00018857451738992824\n",
            "7 0.00030601606097316157\n",
            "8 2.1093067037134933e-05\n",
            "9 0.0002843941405870295\n",
            "10 0.00010763721029009728\n",
            "11 2.7061945045033458e-05\n",
            "12 7.810380028360436e-06\n",
            "13 9.342048500044516e-06\n",
            "14 8.810325566937201e-06\n",
            "15 7.601067781136139e-06\n",
            "16 6.1592874760422775e-06\n",
            "17 5.445440779366506e-06\n",
            "18 4.923857887243971e-06\n",
            "19 4.511405861441344e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6I2UDHGQy7c",
        "colab_type": "code",
        "outputId": "1a622ac2-5b62-447e-fcaf-d7a4e4c49732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([299, 326, 304, 317, 305, 274, 286, 337, 262, 290]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apHXOBwRQ60d",
        "colab_type": "code",
        "outputId": "e09e6a29-7150-4e20-ce91-71b06aa9f62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.33333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH1c6GZJOpUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}